# 必要なライブラリをインポートします
import pandas as pd
import numpy as np

# markdownセル
"""
# 1. データクリーニング：データの理解と前処理

データ分析の最初のステップは、データを理解し、きれいにすることです。これは「データクリーニング」と呼ばれます。
このステップは非常に重要で、以下の理由があります：

1. データの品質を向上させる：誤りや欠損値を修正し、データの信頼性を高めます。
2. 分析の準備：データを一貫した形式に整え、後続の分析作業をスムーズに行えるようにします。
3. データの特徴を理解する：データの内容や構造を把握し、分析の方向性を決める手がかりを得ます。

このノートブックでは、自動車メーカーの鋳造工程における渦流探傷データを扱います。
目的は、欠陥（鋳巣）の発生を減少させるための鋳造条件の探索と、不良品（亀裂のある製品）の検出・判別方法の改善です。
"""

# markdownセル
"""
## データ型ごとの列の一覧表示関数の定義

まず、データの中にどんな種類の情報があるかを確認するための関数を作ります。
この関数は、データの中の列（項目）を種類ごとに分けて表示してくれます。

データ型を知ることは重要です。なぜなら：
1. 適切な分析手法の選択：数値データと文字データでは、使える分析手法が異なります。
2. データの整合性確認：想定外のデータ型があれば、データの問題を早期に発見できます。
3. メモリ使用の最適化：適切なデータ型を使うことで、効率的なデータ処理が可能になります。
"""

# コードセル
# データ型ごとの列の一覧を表示する関数
def display_columns_types(df):
    print("\nデータ型ごとの列の一覧")
    data_types = df.dtypes.value_counts()
    for dtype in data_types.index:
        print(f"\n{dtype}型の列:")
        print(df.select_dtypes(include=[dtype]).columns.tolist())
    print(f"\nデータフレームの形状: {df.shape}")

# markdownセル
"""
## データの前処理

次に、データを分析しやすい形に整えていきます。これは以下の手順で行います：

1. 不要なデータの削除：分析に役立たない情報を取り除きます。
2. データ形式の統一：日付や時間など、同じ種類の情報を一貫した形式に揃えます。
3. 欠損値の処理：データが欠けている部分を適切に扱います。

これらの処理を行う理由は：
- 分析の精度向上：ノイズとなる不要なデータを除去することで、より正確な分析が可能になります。
- 処理の効率化：データ形式を統一することで、後続の分析作業がスムーズになります。
- エラーの防止：欠損値を適切に処理することで、分析時のエラーを防ぎます。
"""

# コードセル
# データを読み込む（ここではdfという変数にデータが既に読み込まれていると仮定します）

# データ型ごとの列の一覧を表示（前処理前）
print("前処理前のデータ構造:")
display_columns_types(df)

# 元のデータフレーム列を保存
original_columns = set(df.columns)

# 全ての値が欠損している列を削除
df = df.dropna(axis=1, how='all')
print("\n全ての値が欠損している列を削除しました。これらの列は分析に役立たないためです。")

# 全ての値が同じ列を削除（比較できないため）
df = df.loc[:, df.round(2).nunique() != 1]
print("\n全ての値が同じ列を削除しました。これらの列は比較や分析に役立たないためです。")

# 削除した列を特定
removed_columns = original_columns - set(df.columns)

# 削除した列を表示
print("\n削除した列:")
for col in removed_columns:
    print(f"- {col}")

# 最初の列名を'image_position'に変更
df = df.rename(columns={df.columns[0]: 'image_position'})
print("\n最初の列名を'image_position'に変更しました。これは列の役割を明確にするためです。")

# 'image_position'列の欠損値を'O'で埋める
df['image_position'] = df['image_position'].fillna('O')
print("\n'image_position'列の欠損値を'O'で埋めました。これは欠損値を意味のある情報に置き換えるためです。")

# "低速変度数(VPC)"の列を数値型に変換
if "低速変度数(VPC)" in df.columns:
    df["低速変度数(VPC)"] = pd.to_numeric(df["低速変度数(VPC)"], errors='coerce')
    print("\n'低速変度数(VPC)'列を数値型に変換しました。これにより数値としての演算や分析が可能になります。")

# 日付と時刻の列を日時型に変換し、新しい日時列を追加
date_columns = ['日付', '出荷検査日付', '加工検査日付']
time_columns = ['時刻', '出荷検査時刻', '加工検査時刻']
datetime_columns = ['日時', '出荷検査日時', '加工検査日時']

for date_col, time_col, datetime_col in zip(date_columns, time_columns, datetime_columns):
    # 日付と時刻を文字列として結合
    datetime_str = df[date_col].astype(str) + ' ' + df[time_col].astype(str)
    # 日時型に変換
    df[datetime_col] = pd.to_datetime(datetime_str, format='%Y-%m-%d %H:%M:%S', errors='coerce')
print("\n日付と時刻の列を結合し、日時型に変換しました。これにより時系列分析が容易になります。")

# 特定の列を数値部分のみの文字列型に変換
id_columns = ['工程内検査1コード', '工程内検査2コード', '加工先1コード', '加工先2コード']
for col in id_columns:
    if col in df.columns:
        df[col] = df[col].apply(lambda x: str(int(x)) if pd.notnull(x) else None)
print("\n特定の列（コード類）を数値部分のみの文字列型に変換しました。これにより一貫した形式でコードを扱えるようになります。")

# データ型ごとの列の一覧を表示（前処理後）
print("\n前処理後のデータ構造:")
display_columns_types(df)

# markdownセル
"""
## 数値データの基本統計量の確認

数値で表されているデータについて、平均値や最大値、最小値などの基本的な統計情報を確認します。
これにより、データの全体的な特徴を把握できます。

この作業が重要な理由：
1. データの範囲や分布の把握：異常値や外れ値の存在を示唆する可能性があります。
2. データの中心傾向の理解：平均値や中央値を知ることで、データの一般的な特徴がわかります。
3. データのばらつきの把握：標準偏差などから、データの散らばり具合を理解できます。

これらの情報は、後続の分析手法の選択や、データの正規化の必要性の判断に役立ちます。
"""

# コードセル
# 数値データの基本統計量を表示
print("数値データの基本統計量:")
display(df.describe())

# markdownセル
"""
## カテゴリデータの確認

文字や記号で表されているデータ（カテゴリデータ）について、それぞれの値がどれくらいの頻度で出現するかを確認します。
これにより、データの分布や特徴を理解できます。

この作業が重要な理由：
1. データの偏りの把握：特定のカテゴリに偏りがあるかどうかを確認できます。
2. 異常値や誤入力の発見：予期しないカテゴリの存在に気づくことができます。
3. エンコーディングの必要性の判断：カテゴリの数や分布によって、適切なエンコーディング方法を選択できます。

これらの情報は、後続の分析手法の選択や、特徴量エンジニアリングの方針決定に役立ちます。
"""

# コードセル
# カテゴリデータの値ごとの出現回数を表示
categorical_columns = df.select_dtypes(include=[object]).columns
for col in categorical_columns:
    print(f"\n{col}の出現回数:")
    display(df[col].value_counts())
    print(f"ユニークな値の数: {df[col].nunique()}")

# markdownセル
"""
## まとめ

以上のデータクリーニングと基本的な分析により、以下のことが達成できました：

1. データの構造と内容の把握
2. 不要なデータの削除とデータ形式の統一
3. 数値データとカテゴリデータの基本的な特徴の理解

次のステップでは、これらの情報を基に、より詳細なデータ分析や特徴量エンジニアリングを行い、
欠陥低減や不良品検出の改善に向けた洞察を得ていきます。
"""
